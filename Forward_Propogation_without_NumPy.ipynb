{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forward Propogation without NumPy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L5JP0HoVCVJ"
      },
      "source": [
        "class Dense():\n",
        "    def __init__(self, datas, weights, bias):\n",
        "        self.datas = datas\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.dense = self.calculate(datas, weights, bias)\n",
        "\n",
        "    def calculate(self, datas, weights, bias):\n",
        "        '''\n",
        "        Here we are calculating the dot product of the input data and weights\n",
        "        as well as adding a bias to the product.\n",
        "\n",
        "        First we are pairing each value in the input tensor with a value in the weights.\n",
        "        Next we are multiplying and summing the values in each data pair and adding a bias to the sum.\n",
        "        '''\n",
        "        weight_pairs = list(zip(*weights))\n",
        "        out_arr = []\n",
        "\n",
        "        for data in datas:\n",
        "            sums = []\n",
        "            for weight in weight_pairs:\n",
        "                data_pairs = list(zip(data, weight))\n",
        "                for bias in biases:\n",
        "                    out_sum = sum([(x*y) for x,y in data_pairs]) + bias\n",
        "                sums.append(out_sum)\n",
        "            out_arr.append(sums)\n",
        "        return out_arr\n",
        "\n",
        "    def relu_activation(self, pre_activation):\n",
        "        '''\n",
        "        ReLu activation will ensure that all values less than 0 from the dot \n",
        "        product calculation will be replaced with 0.\n",
        "        '''\n",
        "        return pre_activation * (pre_activation > 0)  \n",
        "\n",
        "    def run(self, datas, weights, bias):\n",
        "        pre_activation = self.calculate(datas, weights, bias)\n",
        "        post_activation = self.relu_activation(pre_activation)\n",
        "        return post_activation\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHAPl_H14nt0",
        "outputId": "01bc1063-52b0-4de4-9c46-cfbd41560d7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "datas = [[1,2], [2,1]]\n",
        "weights = [[1,2,3], [3,2,1], [4,5,6]]\n",
        "biases = [2,2,2]\n",
        "\n",
        "forward_propagation = Dense(datas, weights, biases)\n",
        "print(forward_propagation.dense)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9, 8, 7], [7, 8, 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}