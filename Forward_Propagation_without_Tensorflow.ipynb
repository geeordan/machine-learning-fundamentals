{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forward Propagation without Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4q8bdAf4fmT"
      },
      "source": [
        "###Forward Propagation without Tensorflow\n",
        "\n",
        "This notebook demonstrates forward propogation with the use of pretrained weights saved in a NumPy format as opposed to a Keras or Tensorflow format.\n",
        "\n",
        "Once the forward propogation has been completed, a comparison between the predicted output and actual output will be conducted and the resulting accuracy will be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5S-tGiSSE5V"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Inference():\n",
        "    def __init__(self, input_data, actual_output):\n",
        "        self.input_data = input_data\n",
        "        self.actual_output = actual_output \n",
        "        self.run_inference = self.run(input_data, actual_output)\n",
        "\n",
        "\n",
        "    def accuracy(self, pred_output, actual_output):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Since pred_output is currently comprised of tensors with float values, \n",
        "        # the values must be converted to one hop format prior to comparison.\n",
        "        one_hot_output = np.argmax(pred_output, axis=1)\n",
        "\n",
        "        for idx, value in enumerate(actual_output):\n",
        "            if value == one_hot_output[idx]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        return (f\"Accuracy: {(correct / total) * 100}%\")\n",
        "\n",
        "\n",
        "    def relu_activation(self, pre_activation):\n",
        "        '''\n",
        "        ReLu activation will ensure that all values less than 0 from the dot \n",
        "        product calculation will be replaced with 0.\n",
        "        '''\n",
        "        return pre_activation * (pre_activation > 0)\n",
        "\n",
        "\n",
        "    def calculate(self, curr_data, weights, bias):\n",
        "        '''\n",
        "        Here we are calculating the dot product of the input data and weights\n",
        "        as well as adding a bias to the product.\n",
        "\n",
        "        First we are pairing each value in the input tensor with a value in the weights.\n",
        "        Next we are multiplying and summing the values in each data pair and adding a bias to the sum.\n",
        "        '''\n",
        "        weight_pairs = list(zip(*weights))\n",
        "        out_arr = []\n",
        "\n",
        "        for data in curr_data:\n",
        "            sums = []\n",
        "            for weight in weight_pairs:\n",
        "                data_pairs = list(zip(data, weight))\n",
        "                out_sum = sum([(x*y) for x,y in data_pairs])\n",
        "                sums.append(out_sum)\n",
        "            out_arr.append(sums)\n",
        "        return out_arr + bias\n",
        "\n",
        "\n",
        "    def run(self, input_data, actual_output):\n",
        "        # Load all weights and biases\n",
        "        d512w = np.load(\"d512w_new.npy\")\n",
        "        d512b = np.load(\"d512b_new.npy\")\n",
        "        d256w = np.load(\"d256w_new.npy\")\n",
        "        d256b = np.load(\"d256b_new.npy\")\n",
        "        d10w = np.load(\"d10w_new.npy\")\n",
        "        d10b = np.load(\"d10b_new.npy\")\n",
        "\n",
        "        # Propagate data through all layers\n",
        "        dense_512 = self.calculate(input_data, d512w, d512b)\n",
        "        dense_512_post = self.relu_activation(dense_512)\n",
        "        dense_256 = self.calculate(dense_512_post, d256w, d256b)\n",
        "        dense_256_post = self.relu_activation(dense_256)\n",
        "        dense_10 = self.calculate(dense_256_post, d10w, d10b)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        find_accuracy = self.accuracy(dense_10, actual_output)\n",
        "        return find_accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX8kHeFXVAzi",
        "outputId": "fe3fd53b-fd6c-47ba-c35b-6e04c41b9c13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The initial Dense layer (Dense(512)) expects a flattened input, therefore the\n",
        "# input data must be reshaped accordingly.\n",
        "x_test_mod = np.load(\"x_test_mod.npy\").reshape(-1, 28*28).astype(\"float32\")\n",
        "y_test_mod = np.load(\"y_test_mod.npy\")\n",
        "\n",
        "forward = Inference(x_test_mod, y_test_mod)\n",
        "print(forward.run_inference)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 98.27%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}